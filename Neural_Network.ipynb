{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcR8pKKRZGSK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network\n",
        "**Perceptron**\n",
        "\n",
        "A perceptron is a type of artificial neural network model that is used for\n",
        "binary classification tasks. It is one of the simplest forms of neural networks and is based on the concept of a biological neuron.\n",
        "\n",
        "The perceptron consists of one or more input nodes, a weight associated with each input, a summation function, an activation function, and a single output node. The input nodes receive input values, which are multiplied by their corresponding weights. These weighted inputs are then summed up, and the result is passed through an activation function to produce the output of the perceptron.\n",
        "\n",
        "The activation function of a perceptron is typically a simple threshold function, such as the step function or the sigmoid function. The threshold function maps the summed value to a binary output, such as 0 or 1, depending on whether the threshold is crossed or not.\n",
        "\n",
        "During the training process, the perceptron adjusts its weights based on the error between its predicted output and the target output. This adjustment is performed using a learning rule, such as the perceptron learning rule or the delta rule, which updates the weights to minimize the error.\n",
        "\n",
        "Perceptrons can be combined to form more complex neural networks, such as multi-layer perceptrons (MLPs), which are capable of solving more complex tasks and performing non-linear classifications. MLPs consist of multiple layers of perceptrons, with each layer connected to the next, forming a feedforward network.\n",
        "\n",
        "The perceptron algorithm and its variants have been widely used in machine learning for tasks such as pattern recognition, image classification, and natural language processing. However, perceptrons have limitations, such as their inability to solve problems that are not linearly separable"
      ],
      "metadata": {
        "id": "X04qxKLRZMG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.activation_func = self._unit_step_func\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _unit_step_func(self, x):\n",
        "        return np.where(x>=0, 1, -1)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                predicted = self.predict(x_i)\n",
        "                update = self.lr * (y[idx] - predicted)\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return self.activation_func(linear_output)"
      ],
      "metadata": {
        "id": "RhLI66xXZQyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we first initialize the Perceptron with a learning rate and a number of iterations. The `_unit_step_func` is the activation function, which is a simple step function that returns 1 if the input is greater than or equal to 0, and -1 otherwise.\n",
        "\n",
        "The `fit` method is used to train the Perceptron. It takes in the features `X` and the target variable `y`. It first initializes the weights and bias to 0, then iterates over the data `n_iters` times. For each iteration, it makes a prediction, calculates the update, and updates the weights and bias.\n",
        "\n",
        "The `predict` method is used to make predictions. It takes in the features `X`, calculates the linear output, and applies the activation function to get the final prediction.\n",
        "\n",
        "You can use this class as follows:"
      ],
      "metadata": {
        "id": "79aPWs8_Zibm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Select only two classes\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Perceptron\n",
        "perceptron = Perceptron()\n",
        "\n",
        "# Train the Perceptron\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = [perceptron.predict(x) for x in X_test]"
      ],
      "metadata": {
        "id": "Ko953ZupZfzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward and backward propagation**"
      ],
      "metadata": {
        "id": "OL7K4op5ZrQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.activation_func = self._unit_step_func\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _unit_step_func(self, x):\n",
        "        return np.where(x>=0, 1, -1)\n",
        "\n",
        "    def _forward_prop(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return self.activation_func(linear_output)\n",
        "\n",
        "    def _backward_prop(self, X, y, y_pred):\n",
        "        error = y - y_pred\n",
        "        self.weights += self.lr * np.dot(X.T, error)\n",
        "        self.bias += self.lr * np.sum(error)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            y_pred = self._forward_prop(X)\n",
        "            self._backward_prop(X, y, y_pred)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self._forward_prop(X)"
      ],
      "metadata": {
        "id": "tAMTZhw3Zpkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `_forward_prop` method, we calculate the linear output and apply the activation function to get the final prediction.\n",
        "\n",
        "In the `_backward_prop` method, we calculate the error, update the weights and bias based on the error, and the learning rate.\n",
        "\n",
        "In the `fit` method, we first initialize the weights and bias to 0, then for each iteration, we make a prediction using forward propagation, calculate the error, and update the weights and bias using backward propagation.\n",
        "\n",
        "The `predict` method is the same as before, it just makes a prediction using forward propagation.\n"
      ],
      "metadata": {
        "id": "FDFUVeZZZ2T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Select only two classes\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Perceptron\n",
        "perceptron = Perceptron()\n",
        "\n",
        "# Train the Perceptron\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = perceptron.predict(X_test)"
      ],
      "metadata": {
        "id": "8KMekB48ZzlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Generate a toy dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(20,)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWxwqcQzZ7Pr",
        "outputId": "37c1bd16-e9ce-43b8-beff-0fcc39fbde0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 1s 41ms/step - loss: 0.7599 - accuracy: 0.5650 - val_loss: 0.5742 - val_accuracy: 0.7200\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5574 - accuracy: 0.7113 - val_loss: 0.4746 - val_accuracy: 0.7650\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4626 - accuracy: 0.7875 - val_loss: 0.4138 - val_accuracy: 0.8200\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.4022 - accuracy: 0.8188 - val_loss: 0.3616 - val_accuracy: 0.8700\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3580 - accuracy: 0.8550 - val_loss: 0.3162 - val_accuracy: 0.8950\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3175 - accuracy: 0.8813 - val_loss: 0.2806 - val_accuracy: 0.9000\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.2854 - accuracy: 0.9038 - val_loss: 0.2504 - val_accuracy: 0.9200\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2586 - accuracy: 0.9212 - val_loss: 0.2266 - val_accuracy: 0.9350\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.2354 - accuracy: 0.9300 - val_loss: 0.2081 - val_accuracy: 0.9450\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.2157 - accuracy: 0.9312 - val_loss: 0.1925 - val_accuracy: 0.9450\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.1925 - accuracy: 0.9450\n",
            "Test accuracy: 0.9449999928474426\n",
            "7/7 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.  We first import the necessary libraries.\n",
        "2.  We then generate a toy dataset using `make_classification` from `sklearn.datasets`. This dataset has 1000 samples, 20 features, 15 informative features, and 5 redundant features.\n",
        "3.  We split the dataset into a training set and a testing set using `train_test_split` from `sklearn.model_selection`.\n",
        "4.  We create a neural network model using `Sequential` from `keras.models`. The model has three layers: an input layer with 20 neurons, a hidden layer with 64 neurons and ReLU activation, another hidden layer with 32 neurons and ReLU activation, and an output layer with 1 neuron and sigmoid activation.\n",
        "5.  We compile the model using `compile` from `keras.models`. We use binary cross-entropy as the loss function, Adam as the optimizer, and accuracy as the metric.\n",
        "6.  We train the model using `fit` from `keras.models`. We train the model for 10 epochs with a batch size of 128.\n",
        "7.  We evaluate the model using `evaluate` from `keras.models`. We print the test accuracy.\n",
        "8.  Finally, we make predictions using `predict` from `keras.models`.\n",
        "\n"
      ],
      "metadata": {
        "id": "P-6PPI4WaIut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adagrad\n",
        "\n",
        "# Generate a toy dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(20,)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with Adagrad optimizer\n",
        "opt = Adagrad(lr=0.01)\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJyHrt5OaICN",
        "outputId": "c8fd75ab-17f5-446a-c8bb-96f7c3431b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adagrad.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 1s 37ms/step - loss: 0.7801 - accuracy: 0.5688 - val_loss: 0.6737 - val_accuracy: 0.5950\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7330 - accuracy: 0.5950 - val_loss: 0.6457 - val_accuracy: 0.6100\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.6989 - accuracy: 0.6125 - val_loss: 0.6233 - val_accuracy: 0.6450\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.6700 - accuracy: 0.6250 - val_loss: 0.6070 - val_accuracy: 0.6550\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6479 - accuracy: 0.6400 - val_loss: 0.5937 - val_accuracy: 0.6850\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.6293 - accuracy: 0.6612 - val_loss: 0.5827 - val_accuracy: 0.6850\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.6135 - accuracy: 0.6725 - val_loss: 0.5729 - val_accuracy: 0.6850\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5994 - accuracy: 0.6850 - val_loss: 0.5642 - val_accuracy: 0.6800\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5874 - accuracy: 0.6975 - val_loss: 0.5562 - val_accuracy: 0.7000\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5763 - accuracy: 0.7013 - val_loss: 0.5492 - val_accuracy: 0.7050\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.5492 - accuracy: 0.7050\n",
            "Test accuracy: 0.7049999833106995\n",
            "7/7 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.  We import the necessary libraries.\n",
        "2.  We generate a toy dataset using `make_classification` from `sklearn.datasets`.\n",
        "3.  We split the dataset into a training set and a testing set using `train_test_split` from `sklearn.model_selection`.\n",
        "4.  We create a neural network model using `Sequential` from `keras.models`.\n",
        "5.  We compile the model using `compile` from `keras.models`. We use binary cross-entropy as the loss function, Adagrad as the optimizer, and accuracy as the metric.\n",
        "6.  We train the model using `fit` from `keras.models`. We train the model for 10 epochs with a batch size of 128.\n",
        "7.  We evaluate the model using `evaluate` from `keras.models`. We print the test accuracy.\n",
        "8.  Finally, we make predictions using `predict` from `keras.models`.\n",
        "\n",
        "The Adagrad optimization algorithm is an extension of stochastic gradient descent that adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. This helps in converging faster and avoiding local minima.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nmF4S4z0aauL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "from keras.optimizers import Adagrad\n",
        "\n",
        "# Generate a toy dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
        "\n",
        "# Reshape the data to 2D for Conv2D\n",
        "X = X.reshape(-1, 4, 5, 1)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a neural network model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(4, 5, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with Adagrad optimizer\n",
        "opt = Adagrad(lr=0.01)\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_NwbCtlaE-8",
        "outputId": "d2994dce-0a40-4ec5-fc45-6146a8127d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adagrad.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 2s 37ms/step - loss: 0.6998 - accuracy: 0.5675 - val_loss: 0.6906 - val_accuracy: 0.5650\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.6770 - accuracy: 0.5987 - val_loss: 0.6733 - val_accuracy: 0.5700\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6590 - accuracy: 0.6112 - val_loss: 0.6595 - val_accuracy: 0.5700\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6442 - accuracy: 0.6350 - val_loss: 0.6474 - val_accuracy: 0.6150\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.6312 - accuracy: 0.6762 - val_loss: 0.6379 - val_accuracy: 0.6550\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6209 - accuracy: 0.6938 - val_loss: 0.6289 - val_accuracy: 0.6700\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6113 - accuracy: 0.7050 - val_loss: 0.6209 - val_accuracy: 0.7050\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.6028 - accuracy: 0.7262 - val_loss: 0.6135 - val_accuracy: 0.7050\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.5947 - accuracy: 0.7387 - val_loss: 0.6070 - val_accuracy: 0.7150\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.5876 - accuracy: 0.7450 - val_loss: 0.6010 - val_accuracy: 0.7150\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6010 - accuracy: 0.7150\n",
            "Test accuracy: 0.7149999737739563\n",
            "7/7 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "\n",
        "1.  We import the necessary libraries.\n",
        "2.  We generate a toy dataset using `make_classification` from `sklearn.datasets`.\n",
        "3.  We reshape the data to 2D for `Conv2D` layer.\n",
        "4.  We split the dataset into a training set and a testing set using `train_test_split` from `sklearn.model_selection`.\n",
        "5.  We create a neural network model using `Sequential` from `keras.models`.\n",
        "6.  We add a `Conv2D` layer with 32 filters, kernel size of (2, 2), and ReLU activation.\n",
        "7.  We add a `Flatten` layer to flatten the output of the `Conv2D` layer.\n",
        "8.  We add three `Dense` layers with 64, 32, and 1 neurons, and ReLU and sigmoid activations.\n",
        "9.  We compile the model using `compile` from `keras.models`. We use binary cross-entropy as the loss function, Adagrad as the optimizer, and accuracy as the metric.\n",
        "10. We train the model using `fit` from `keras.models`. We train the model for 10 epochs with a batch size of 128.\n",
        "11. We evaluate the model using `evaluate` from `keras.models`. We print the test accuracy.\n",
        "12. Finally, we make predictions using `predict` from `keras.models`.\n",
        "\n",
        "The 2D convolution algorithm is added using the `Conv2D` layer. This layer performs 2D convolution on the input data, which is useful for image data. The `Flatten` layer is used to flatten the output of the `Conv2D` layer, which is necessary for the `Dense` layers."
      ],
      "metadata": {
        "id": "bMr4pUSHguty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Conv3D, Flatten\n",
        "from keras.optimizers import Adagrad\n",
        "\n",
        "# Generate a toy dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
        "\n",
        "# Reshape the data to 3D for Conv3D\n",
        "X = X.reshape(-1, 2, 2, 5, 1)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a neural network model\n",
        "model = Sequential()\n",
        "model.add(Conv3D(32, kernel_size=(1, 1, 2), activation='relu', input_shape=(2, 2, 5, 1)))\n",
        "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with Adagrad optimizer\n",
        "opt = Adagrad(lr=0.01)\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdnfnXkpgtuS",
        "outputId": "4e6a63e4-bf98-4443-edcd-8068d1e25e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adagrad.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 1s 46ms/step - loss: 0.6965 - accuracy: 0.4775 - val_loss: 0.6967 - val_accuracy: 0.4850\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.6928 - accuracy: 0.4950 - val_loss: 0.6935 - val_accuracy: 0.5100\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6891 - accuracy: 0.5163 - val_loss: 0.6906 - val_accuracy: 0.5350\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6857 - accuracy: 0.5300 - val_loss: 0.6877 - val_accuracy: 0.5400\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.6824 - accuracy: 0.5600 - val_loss: 0.6850 - val_accuracy: 0.5450\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.6792 - accuracy: 0.5750 - val_loss: 0.6822 - val_accuracy: 0.5550\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.6760 - accuracy: 0.6000 - val_loss: 0.6794 - val_accuracy: 0.5950\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.6728 - accuracy: 0.6137 - val_loss: 0.6766 - val_accuracy: 0.6200\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.6697 - accuracy: 0.6250 - val_loss: 0.6738 - val_accuracy: 0.6150\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.6666 - accuracy: 0.6375 - val_loss: 0.6710 - val_accuracy: 0.6250\n",
            "7/7 [==============================] - 0s 3ms/step - loss: 0.6710 - accuracy: 0.6250\n",
            "Test accuracy: 0.625\n",
            "7/7 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "\n",
        "1.  We import the necessary libraries.\n",
        "2.  We generate a toy dataset using `make_classification` from `sklearn.datasets`.\n",
        "3.  We reshape the data to 3D for `Conv3D` layer.\n",
        "4.  We split the dataset into a training set and a testing set using `train_test_split` from `sklearn.model_selection`.\n",
        "5.  We create a neural network model using `Sequential` from `keras.models`.\n",
        "6.  We add a `Conv3D` layer with 32 filters, kernel size of (1, 1, 2), and ReLU activation.\n",
        "7.  We add a `Conv2D` layer with 32 filters, kernel size of (2, 2), and ReLU activation.\n",
        "8.  We add a `Flatten` layer to flatten the output of the convolutional layers.\n",
        "9.  We add three `Dense` layers with 64, 32, and 1 neurons, and ReLU and sigmoid activations.\n",
        "10. We compile the model using `compile` from `keras.models`. We use binary cross-entropy as the loss function, Adagrad as the optimizer, and accuracy as the metric.\n",
        "11. We train the model using `fit` from `keras.models`. We train the model for 10 epochs with a batch size of 128.\n",
        "12. We evaluate the model using `evaluate` from `keras.models`. We print the test accuracy.\n",
        "13. Finally, we make predictions using `predict` from `keras.models`.\n",
        "\n",
        "The 3D convolution algorithm is added using the `Conv3D` layer. This layer performs 3D convolution on the input data, which is useful for 3D image data. The `Conv2D` layer is used for 2D convolution, and the `Flatten` layer is used to flatten the output of the convolutional layers, which is necessary for the `Dense` layers."
      ],
      "metadata": {
        "id": "pmLZbImAiLjD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLhcioGViNBV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}